---
title: "Hard-Coded Gaussian Attention for Neural Machine Translation"
collection: publications
permalink: /publication/2019-12-10-hard-coded-gaussian-attention-for-nmt-number-1
excerpt: 'This paper is about using hard-coded Gaussian Attention instead of learned Multi-headed Attention in Transformer in Neural Machine Translation.'
date: 2019-12-10
venue: 'Submitted to ACL 2020'

---
This paper is currently under review.
